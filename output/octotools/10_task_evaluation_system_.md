# Chapter 10: Task Evaluation System

Welcome to the final chapter of the `octotools` tutorial! In the last chapter, [Context Verification](09_context_verification_.md), we learned how `octotools` determines when it has gathered enough information to answer your question by checking its internal memory against the original request. But once `octotools` gives you that final answer, how do we know if it's actually *correct* or *good*? How can we measure how well the system is performing overall? That's where the **Task Evaluation System** comes in.

**What's the Problem? Grading the Homework**

Imagine `octotools` is a student trying to solve different kinds of problems â€“ math questions, questions about pictures, logic puzzles, and more. It tries its best using all the components we've learned about: the [Solver Framework](01_solver_framework_.md), the [Planning-Execution Cycle](02_planning_execution_cycle_.md), various [Tools](05_tool_architecture_.md), and the [LLM Engine Integration](06_llm_engine_integration_.md).

After the student submits their homework (the answers from `octotools`), how do we, as developers or researchers, grade it? We need a way to compare the student's answers against the actual correct answers. We also need a standardized way to score the performance across different types of assignments (benchmarks).

The **Task Evaluation System** in `octotools` is essentially this grading framework. It's not typically used during the normal operation of asking `octotools` a single question. Instead, it's a set of tools and scripts used *afterwards* by developers to:
1.  Run `octotools` on a large set of predefined problems (called **benchmarks**) where the correct answers are already known.
2.  Compare the answers generated by `octotools` against these known correct answers (the "ground truth").
3.  Calculate **performance metrics** (like accuracy) to understand how well `octotools` performed on that specific set of tasks.

This helps us understand the system's strengths and weaknesses, track improvements over time, and compare different versions or configurations (like trying different AI models).

**Key Concepts**

1.  **Benchmarks:** These are like standardized tests for AI systems. They consist of a collection of problems (questions, often with images or other data) and their corresponding correct answers. Examples relevant to `octotools` might include:
    *   **MathVista:** Mathematical reasoning with visual elements.
    *   **CLEVR-Math:** Answering math questions based on images of objects.
    *   **VQA-v2:** Answering general questions about images.
    *   *(Others for different capabilities like reasoning, coding, etc.)*

2.  **Ground Truth:** This refers to the known, correct answers provided within a benchmark dataset.

3.  **Results:** These are the answers generated by `octotools` when run on the problems in a benchmark dataset.

4.  **Scoring Scripts:** These are specialized Python scripts designed for each benchmark. They contain the logic to:
    *   Load the benchmark questions and ground truth answers.
    *   Load the results generated by `octotools`.
    *   Intelligently **extract** the core answer from `octotools`' potentially conversational output.
    *   **Normalize** both the extracted answer and the ground truth answer to a common format.
    *   **Compare** the normalized answers.
    *   Calculate overall **metrics** like accuracy.

5.  **Performance Metrics:** These are numbers that summarize the system's performance. The most common is **Accuracy** (the percentage of questions answered correctly), but others might include average processing time, average number of steps taken, or how often specific tools were used.

**How Evaluation Works (The Grading Process)**

Evaluating `octotools` is typically a multi-step process performed offline by developers:

1.  **Run on Benchmark:** Use the `octotools` [Solver](01_solver_framework_.md) to process all the questions in a specific benchmark dataset (e.g., all questions in the MathVista test set). Save the detailed output (including the final answer, often the `direct_output`) for each question into result files (usually JSON format), typically one file per question.
2.  **Execute Scoring Script:** Run the specific scoring script designed for that benchmark. This script needs access to:
    *   The original benchmark data file (containing questions and ground truth answers).
    *   The directory containing all the result files generated by `octotools` in step 1.
3.  **Analyze Results:** The script performs the extraction, normalization, and comparison for each question, calculates the overall accuracy, and often saves a detailed report and a final score summary file.

**How to Use (Running an Evaluation)**

As a typical user asking `octotools` questions, you won't directly use the Task Evaluation System. This is primarily a tool for developers. If you were developing `octotools` and wanted to test its performance on the MathVista benchmark, you would:

1.  First, run `octotools` on all the MathVista questions and save the outputs (e.g., in a directory called `results/mathvista_run1/`).
2.  Then, use the command line to run the MathVista scoring script (like the `calculate_score.py` provided in the context).

Here's a conceptual command:

```bash
# Example command to run the MathVista evaluation script
python tasks/mathvista/calculate_score.py \
    --data_file data/benchmarks/mathvista/testmini.json \
    --result_dir results/mathvista_run1/ \
    --output_file mathvista_run1_scored_results.json \
    --response_type direct_output
```

*   `python tasks/mathvista/calculate_score.py`: Tells Python to execute the scoring script for MathVista.
*   `--data_file ...`: Specifies the path to the benchmark data file containing questions and correct answers.
*   `--result_dir ...`: Specifies the directory where `octotools` saved its output files (one per question).
*   `--output_file ...`: Specifies the name for the detailed file containing the score for each question.
*   `--response_type direct_output`: Tells the script which field in the result files contains the answer generated by `octotools` (it could be `final_output`, `direct_output`, etc.).

Running this command would process all the results, print the final accuracy score, and save the detailed scoring information.

**Under the Hood: Inside the Scoring Scripts**

The Task Evaluation System relies heavily on these benchmark-specific scoring scripts. Let's look at the common steps inside scripts like `tasks/mathvista/calculate_score.py`, `tasks/clevr-math/calculate_score.py`, or `tasks/vqav2/calculate_score.py`.

1.  **Load Data:** The script starts by loading the benchmark questions/answers and the results produced by `octotools`.

    ```python
    # Simplified concept: Loading data
    import json
    import os

    # Load benchmark data (questions + ground truth answers)
    with open(args.data_file, 'r') as f:
        benchmark_data = {data["pid"]: data for data in json.load(f)}

    # Load octotools results from the specified directory
    results = {}
    for file in os.listdir(args.result_dir):
        if file.endswith(".json") and "output_" in file:
            # ... (code to load each result file) ...
            pid = ... # Get problem ID
            result_content = json.load(f)
            results[pid] = benchmark_data[pid] # Combine benchmark data
            # Get the answer generated by octotools
            results[pid]["response"] = result_content[args.response_type]
            results[pid]["correct_answer"] = benchmark_data[pid]["answer"]
    ```
    This code prepares two main data structures: one holding the original benchmark info (including correct answers) and another holding the answers produced by `octotools`.

2.  **Iterate and Score:** The script then loops through each problem ID (`pid`). Inside the loop, it performs the core scoring logic:

    *   **Answer Extraction:** `octotools` might output "The final answer is 14." or just "14". The script needs to extract the essential part. This can be complex, sometimes even using another LLM call just for extraction!

        ```python
        # Simplified from tasks/mathvista/calculate_score.py - extract_answer
        def extract_answer(response, problem, quick_extract=False):
            # ... (try simple type conversions first, e.g., int(), float()) ...

            # If complex, use an LLM to extract the answer based on examples
            if not extraction_successful:
                try:
                    # Use a helper LLM engine (part of evaluation toolkit)
                    llm_engine_for_scoring = create_llm_engine("gpt-4o-mini")
                    # Provide examples (demo_prompt) of how to extract
                    full_prompt = create_test_prompt(demo_prompt, problem['query'], response)
                    extraction = llm_engine_for_scoring(full_prompt)
                    return extraction
                except Exception as e:
                    print(f"LLM extraction failed: {e}")
                    return "" # Failed extraction
            # ...
        ```
        This shows how scoring might involve rule-based extraction or even calling an [LLM Engine](06_llm_engine_integration_.md) with specific examples (`demo_prompt`) to intelligently pull out the answer from a messy response.

    *   **Answer Normalization:** Answers might need cleaning. For multiple choice, "B" needs to be treated the same as "(B)" or the actual text of option B. Numbers might need rounding.

        ```python
        # Simplified from tasks/mathvista/calculate_score.py - normalize_extracted_answer
        def normalize_extracted_answer(extraction, question_data):
            q_type = question_data["question_type"]
            a_type = question_data["answer_type"]
            choices = question_data["choices"]

            if q_type == 'multi_choice':
                # ... (logic to handle "(A)" vs "A" vs choice text) ...
                # Find the most similar valid option if needed
                extraction = get_most_similar(extraction, choices)
            elif a_type == 'integer':
                # Convert to integer string, handle errors
                try: extraction = str(int(float(extraction)))
                except: extraction = None
            elif a_type == 'float':
                # Round to required precision, handle errors
                try: extraction = str(round(float(extraction), question_data["precision"]))
                except: extraction = None
            # ...
            return extraction
        ```
        This step ensures we compare apples to apples by converting both the system's answer and the ground truth answer to a standard format.

    *   **Comparison:** Finally, the script checks if the normalized extracted answer matches the normalized ground truth answer.

        ```python
        # Simplified comparison logic
        normalized_answer = normalize_extracted_answer(extracted_answer, question_data)
        correct_answer_normalized = normalize_extracted_answer(question_data["correct_answer"], question_data)

        # Simple equality check (might be more complex for semantic comparison)
        is_correct = (normalized_answer == correct_answer_normalized)

        # Store if this question was answered correctly
        results[pid]["true_false"] = is_correct
        ```
        This is the actual "grading" step where the system's answer is marked right or wrong. Note: For some tasks (like VQA), the comparison might need another LLM call to check for *semantic* similarity rather than exact string matching (see `tasks/vqav2/calculate_score.py`).

3.  **Calculate Metrics:** After looping through all questions, the script calculates the overall accuracy and potentially other statistics using helper classes like `ResultAnalyzer`.

    ```python
    # Simplified calculation and reporting
    correct_count = sum(1 for pid in results if results[pid]["true_false"])
    total_count = len(results)
    accuracy = round(correct_count / total_count * 100, 2)

    print(f"\nAccuracy: {accuracy}% ({correct_count}/{total_count})")

    # Save final scores and detailed results
    scores = {"accuracy": accuracy, "correct": correct_count, "total": total_count}
    # ... (potentially add step/time/tool usage stats from ResultAnalyzer) ...
    with open(score_file, 'w') as f:
        json.dump(scores, f, indent=4)
    ```
    This final part aggregates the individual question scores into overall performance metrics.

**Conclusion**

Congratulations on reaching the end of the `octotools` tutorial! In this final chapter, we explored the **Task Evaluation System**. While not part of the core runtime loop, it's a crucial framework for developers to measure how well `octotools` performs. By running the system on standardized benchmarks (like MathVista or VQA) and using specialized scoring scripts to compare its answers against the known ground truth, we can calculate performance metrics like accuracy. This systematic evaluation helps identify areas for improvement and track progress as `octotools` continues to evolve.

Throughout this tutorial, we've journeyed from the high-level [Solver Framework](01_solver_framework_.md) and its [Planning-Execution Cycle](02_planning_execution_cycle_.md), through understanding queries ([Query Analysis](03_query_analysis_.md)), handling images ([Multimodal Processing](04_multimodal_processing_.md)), using specialized [Tools](05_tool_architecture_.md), connecting to AI brains ([LLM Engine Integration](06_llm_engine_integration_.md)), generating precise instructions ([Tool Command Generation](07_tool_command_generation_.md)), remembering progress ([Memory Management](08_memory_management_.md)), knowing when to stop ([Context Verification](09_context_verification_.md)), and finally, assessing the quality of the results ([Task Evaluation System](10_task_evaluation_system_.md)).

We hope this step-by-step guide has given you a clear, beginner-friendly understanding of the core concepts behind `octotools`. Happy coding!

---

Generated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)